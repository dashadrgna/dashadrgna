{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dashadrgna/dashadrgna/blob/master/Deryagina_L8_1_NLP_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HdBQ76EnLfhL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce58d44-b24b-4b2f-81b9-c9b758e13652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elkXpTy4yvj7"
      },
      "source": [
        "# Токенизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymqfrOwnIIMQ"
      },
      "source": [
        "## Токенизация по предложениям"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rorGJPvAxAtw"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"There was clearly nothing to do but flop down on the shabby little\n",
        "couch and howl. So Della did it.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kftSWEQJxHTG",
        "outputId": "8292ccfd-32c5-4df0-bd48-b7eaa78167bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['There was clearly nothing to do but flop down on the shabby little\\ncouch and howl.',\n",
              " 'So Della did it.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UiOvWY4iJGRb"
      },
      "outputs": [],
      "source": [
        "text_2 = \"\"\"Один доллар восемьдесят семь центов. Это было все. Из них шестьдесят центов монетками по одному центу. За каждую из этих\n",
        " монеток пришлось торговаться с бакалейщиком, зеленщиком, мясником так, что даже уши горели от безмолвного неодобрения,\n",
        "которое вызывала подобная бережливость.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQKH0SgsJQkl",
        "outputId": "7bc05f31-5ea0-4cb6-c061-af2192ad69ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Один доллар восемьдесят семь центов.',\n",
              " 'Это было все.',\n",
              " 'Из них шестьдесят центов монетками по одному центу.',\n",
              " 'За каждую из этих\\n монеток пришлось торговаться с бакалейщиком, зеленщиком, мясником так, что даже уши горели от безмолвного неодобрения,\\nкоторое вызывала подобная бережливость.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "nltk.sent_tokenize(text_2, language='russian')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZQa_yFkIMhG"
      },
      "source": [
        "## Токенизация по словам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG5opYSunjB2",
        "outputId": "1a3206e4-3824-4b5e-848d-24b47f8f83c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'was', 'clearly', 'nothing', 'to', 'do', 'but', 'flop', 'down', 'on', 'the', 'shabby', 'little', 'couch', 'and', 'howl', '.', 'So', 'Della', 'did', 'it', '.']\n"
          ]
        }
      ],
      "source": [
        "en_tokens = nltk.word_tokenize(text)\n",
        "print(en_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "G2nj1_T0tT_V"
      },
      "outputs": [],
      "source": [
        "sentence = \"Della finished her cry and attended to her cheeks with the powder rag.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIZy4ymtnsH4",
        "outputId": "34d03a99-1337-423b-9c17-32a2615975a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Della', 'finished', 'her', 'cry', 'and', 'attended', 'to', 'her', 'cheeks', 'with', 'the', 'powder', 'rag', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = nltk.wordpunct_tokenize(sentence)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1sNGin2ofoa",
        "outputId": "af5b6aff-a77f-48f0-cde7-7704795a4e68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 5),\n",
              " (6, 14),\n",
              " (15, 18),\n",
              " (19, 22),\n",
              " (23, 26),\n",
              " (27, 35),\n",
              " (36, 38),\n",
              " (39, 42),\n",
              " (43, 49),\n",
              " (50, 54),\n",
              " (55, 58),\n",
              " (59, 65),\n",
              " (66, 69),\n",
              " (69, 70)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer as twt\n",
        "list(twt().span_tokenize(sentence))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fYU1vqBuonAy"
      },
      "outputs": [],
      "source": [
        "sentence_2 = \"Делла рыскала по магазинам в поисках подарка для Джима.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v8yw9sqpC23",
        "outputId": "14d98a3e-8f82-4eb4-fff8-366243b6d9e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Делла', 'рыскала', 'по', 'магазинам', 'в', 'поисках', 'подарка', 'для', 'Джима', '.']\n"
          ]
        }
      ],
      "source": [
        "ru_tokens = nltk.word_tokenize(sentence_2, language='russian')\n",
        "print(ru_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHOLVdblky_Q"
      },
      "source": [
        "# Фильтрация по стоп-словам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X_uyEYxkxTK",
        "outputId": "215134c5-9c35-49e4-dec1-53f602f0fe60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs05WCqJuRPx",
        "outputId": "bea0e193-4238-48e7-d56c-59a9a1da4b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'was', 'clearly', 'nothing', 'to', 'do', 'but', 'flop', 'down', 'on', 'the', 'shabby', 'little', 'couch', 'and', 'howl', '.', 'So', 'Della', 'did', 'it', '.']\n"
          ]
        }
      ],
      "source": [
        "print(en_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u2FHod9tvTZ",
        "outputId": "eacac2cb-6d73-4340-d088-a74f173f6904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'clearly', 'nothing', 'flop', 'shabby', 'little', 'couch', 'howl', '.', 'So', 'Della', '.']\n"
          ]
        }
      ],
      "source": [
        "filtered_tokens = [i for i in en_tokens if i not in stopwords]\n",
        "print(filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8yIYsXGA2ca"
      },
      "source": [
        "# Нормализация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRpy34lWShZP"
      },
      "source": [
        "## Английский язык"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF0u9KrJSkiC"
      },
      "source": [
        "### Стемминг"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpkkIQMkEV4t",
        "outputId": "04e44d81-10f8-42ca-8615-941345fdecc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there\n",
            "wa\n",
            "clearli\n",
            "noth\n",
            "to\n",
            "do\n",
            "but\n",
            "flop\n",
            "down\n",
            "on\n",
            "the\n",
            "shabbi\n",
            "littl\n",
            "couch\n",
            "and\n",
            "howl\n",
            ".\n",
            "So\n",
            "della\n",
            "did\n",
            "it\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "p_stemmer = nltk.PorterStemmer()\n",
        "\n",
        "for word in en_tokens:\n",
        "     print(p_stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plwNa0KrSnwR"
      },
      "source": [
        "### Лемматизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWzj_TfTFPsr",
        "outputId": "282d706a-796c-4117-dee3-2be6e021cdbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "There\n",
            "wa\n",
            "clearly\n",
            "nothing\n",
            "to\n",
            "do\n",
            "but\n",
            "flop\n",
            "down\n",
            "on\n",
            "the\n",
            "shabby\n",
            "little\n",
            "couch\n",
            "and\n",
            "howl\n",
            ".\n",
            "So\n",
            "Della\n",
            "did\n",
            "it\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
        "\n",
        "for word in en_tokens:\n",
        "    print(wordnet_lemmatizer.lemmatize(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M30puW0RSraI"
      },
      "source": [
        "## Русский язык"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfTPUvskBQHu",
        "outputId": "758d3284-23c4-443b-b717-fa3e29f91fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 8.6 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XB5D3EBzA4Vh"
      },
      "outputs": [],
      "source": [
        "import pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjInpZhoBVEk",
        "outputId": "fd015051-c657-4375-c665-cfe373312d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "делла -> делла\n",
            "рыскала -> рыскать\n",
            "по -> по\n",
            "магазинам -> магазин\n",
            "в -> в\n",
            "поисках -> поиск\n",
            "подарка -> подарок\n",
            "для -> для\n",
            "джима -> джим\n",
            "джима -> джим\n",
            ". -> .\n"
          ]
        }
      ],
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "for token in ru_tokens:\n",
        "    ps = morph.parse(token)\n",
        "    # print(ps)\n",
        "\n",
        "    max_score = max([p.score for p in ps])\n",
        "\n",
        "    for p in ps:\n",
        "      if p.score == max_score:\n",
        "        print(p.word, '->', p.normal_form)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "l8-1 NLP Preprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}